{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge-score","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:36:58.834668Z","iopub.execute_input":"2023-09-11T01:36:58.835087Z","iopub.status.idle":"2023-09-11T01:37:10.318877Z","shell.execute_reply.started":"2023-09-11T01:36:58.835055Z","shell.execute_reply":"2023-09-11T01:37:10.317614Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer,GPT2LMHeadModel\n# Instantiating the model and tokenizer with gpt\ntokenizer = GPT2Tokenizer.from_pretrained('microsoft/BioGPT-Large')\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel = GPT2LMHeadModel.from_pretrained('microsoft/BioGPT-Large')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:10.321212Z","iopub.execute_input":"2023-09-11T01:37:10.321522Z","iopub.status.idle":"2023-09-11T01:37:46.466293Z","shell.execute_reply.started":"2023-09-11T01:37:10.321499Z","shell.execute_reply":"2023-09-11T01:37:46.464478Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'BioGptTokenizer'. \nThe class this function is called from is 'GPT2Tokenizer'.\nYou are using a model of type biogpt to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at microsoft/BioGPT-Large and are newly initialized: ['h.29.ln_2.weight', 'h.47.attn.c_attn.weight', 'h.16.ln_2.weight', 'h.30.ln_2.weight', 'h.7.attn.c_attn.weight', 'h.12.mlp.c_proj.bias', 'h.3.mlp.c_proj.weight', 'h.39.attn.c_proj.weight', 'h.2.mlp.c_proj.weight', 'h.0.ln_2.bias', 'h.44.ln_2.bias', 'h.14.mlp.c_fc.bias', 'h.37.attn.c_attn.weight', 'h.40.ln_1.weight', 'h.15.ln_1.weight', 'h.8.mlp.c_fc.bias', 'h.15.mlp.c_fc.weight', 'h.17.attn.c_proj.bias', 'h.26.ln_2.weight', 'h.47.ln_2.bias', 'h.3.ln_2.weight', 'h.11.attn.c_attn.weight', 'h.37.attn.c_attn.bias', 'h.29.attn.c_attn.bias', 'wpe.weight', 'h.19.attn.c_attn.bias', 'h.12.ln_2.bias', 'h.42.ln_2.weight', 'h.41.mlp.c_fc.bias', 'h.42.attn.c_attn.weight', 'h.15.ln_2.bias', 'h.25.ln_1.weight', 'h.12.mlp.c_proj.weight', 'h.44.attn.c_proj.bias', 'h.4.attn.c_proj.weight', 'h.44.mlp.c_proj.weight', 'h.14.attn.c_proj.weight', 'h.25.ln_2.weight', 'h.27.mlp.c_fc.bias', 'h.41.mlp.c_fc.weight', 'h.40.ln_2.weight', 'h.18.mlp.c_fc.bias', 'h.1.mlp.c_fc.bias', 'h.17.mlp.c_fc.bias', 'h.5.attn.c_attn.bias', 'h.15.attn.c_proj.weight', 'h.45.mlp.c_fc.bias', 'h.40.ln_2.bias', 'h.45.mlp.c_proj.weight', 'h.35.mlp.c_fc.weight', 'h.39.attn.c_proj.bias', 'h.14.ln_2.weight', 'h.46.mlp.c_fc.weight', 'h.37.attn.c_proj.bias', 'h.29.ln_2.bias', 'h.29.attn.c_proj.bias', 'h.44.attn.c_proj.weight', 'h.28.ln_2.bias', 'h.8.attn.c_attn.bias', 'h.37.attn.c_proj.weight', 'h.14.mlp.c_proj.weight', 'h.38.ln_1.weight', 'h.37.mlp.c_fc.weight', 'h.29.mlp.c_fc.bias', 'h.43.mlp.c_fc.weight', 'h.5.ln_1.bias', 'h.28.attn.c_proj.bias', 'h.24.ln_2.bias', 'h.3.mlp.c_fc.bias', 'h.12.attn.c_attn.bias', 'h.47.mlp.c_proj.bias', 'h.45.mlp.c_fc.weight', 'h.29.ln_1.weight', 'h.29.attn.c_proj.weight', 'h.7.ln_1.bias', 'h.36.attn.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.35.attn.c_proj.bias', 'h.22.mlp.c_proj.weight', 'h.24.attn.c_proj.bias', 'h.18.ln_2.weight', 'h.43.mlp.c_proj.weight', 'h.9.ln_1.weight', 'h.17.attn.c_attn.weight', 'h.33.ln_1.weight', 'h.21.mlp.c_fc.bias', 'lm_head.weight', 'h.38.ln_1.bias', 'h.27.mlp.c_proj.bias', 'h.6.attn.c_proj.weight', 'h.13.ln_2.bias', 'h.29.ln_1.bias', 'h.35.attn.c_attn.weight', 'h.30.mlp.c_fc.weight', 'h.14.attn.c_attn.weight', 'h.12.ln_1.weight', 'h.32.attn.c_attn.weight', 'h.8.ln_1.bias', 'h.23.mlp.c_fc.bias', 'h.31.attn.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.5.mlp.c_fc.bias', 'h.45.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.27.mlp.c_proj.weight', 'h.21.attn.c_proj.bias', 'h.38.ln_2.weight', 'h.40.attn.c_proj.weight', 'h.43.attn.c_proj.weight', 'h.22.ln_2.weight', 'h.14.mlp.c_fc.weight', 'h.32.attn.c_attn.bias', 'h.37.mlp.c_proj.weight', 'h.26.ln_2.bias', 'h.10.ln_1.weight', 'h.39.mlp.c_proj.bias', 'h.47.ln_1.weight', 'h.24.mlp.c_fc.bias', 'h.16.ln_1.bias', 'h.18.mlp.c_proj.bias', 'h.3.ln_1.bias', 'wte.weight', 'h.20.mlp.c_proj.bias', 'h.19.mlp.c_proj.bias', 'h.39.attn.c_attn.weight', 'h.0.attn.c_attn.weight', 'h.10.mlp.c_proj.bias', 'h.22.attn.c_attn.bias', 'h.25.attn.c_attn.weight', 'h.41.attn.c_proj.weight', 'h.46.mlp.c_proj.bias', 'h.13.ln_2.weight', 'ln_f.bias', 'h.29.mlp.c_fc.weight', 'h.31.attn.c_attn.weight', 'h.14.ln_2.bias', 'h.47.attn.c_proj.bias', 'h.21.attn.c_proj.weight', 'h.8.ln_2.bias', 'h.43.ln_2.bias', 'h.34.ln_2.weight', 'h.6.mlp.c_proj.bias', 'h.1.mlp.c_proj.bias', 'h.24.attn.c_attn.weight', 'h.44.ln_1.weight', 'h.27.attn.c_proj.bias', 'h.18.ln_2.bias', 'h.7.mlp.c_fc.bias', 'h.3.attn.c_proj.bias', 'h.28.mlp.c_fc.bias', 'h.41.attn.c_attn.weight', 'h.18.attn.c_attn.bias', 'h.26.ln_1.weight', 'h.33.mlp.c_fc.weight', 'h.34.attn.c_attn.bias', 'h.8.mlp.c_fc.weight', 'h.44.mlp.c_proj.bias', 'h.11.ln_2.weight', 'h.34.mlp.c_fc.weight', 'h.10.mlp.c_fc.bias', 'h.34.mlp.c_proj.bias', 'h.46.ln_1.weight', 'h.47.mlp.c_proj.weight', 'h.22.mlp.c_proj.bias', 'h.4.attn.c_attn.weight', 'h.47.ln_2.weight', 'h.42.ln_1.weight', 'h.22.mlp.c_fc.bias', 'h.9.mlp.c_fc.bias', 'h.9.ln_1.bias', 'h.36.attn.c_attn.bias', 'h.45.ln_1.weight', 'h.23.attn.c_proj.bias', 'h.38.ln_2.bias', 'h.35.ln_2.weight', 'h.38.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.25.attn.c_attn.bias', 'h.13.mlp.c_fc.bias', 'h.11.attn.c_proj.weight', 'h.34.ln_1.weight', 'h.32.mlp.c_proj.bias', 'h.35.mlp.c_fc.bias', 'h.1.attn.c_attn.bias', 'h.39.attn.c_attn.bias', 'h.7.mlp.c_proj.weight', 'h.0.ln_2.weight', 'h.46.ln_2.bias', 'h.46.mlp.c_fc.bias', 'h.28.mlp.c_proj.weight', 'h.4.ln_1.weight', 'h.9.attn.c_attn.bias', 'h.33.attn.c_attn.bias', 'h.24.attn.c_proj.weight', 'h.33.mlp.c_proj.bias', 'h.13.mlp.c_proj.bias', 'h.39.ln_2.weight', 'h.2.attn.c_attn.bias', 'h.45.attn.c_attn.weight', 'h.16.mlp.c_fc.bias', 'h.35.ln_1.weight', 'h.21.attn.c_attn.weight', 'h.47.mlp.c_fc.bias', 'h.11.attn.c_attn.bias', 'h.36.mlp.c_fc.weight', 'h.4.mlp.c_proj.weight', 'h.18.attn.c_proj.bias', 'h.23.attn.c_attn.bias', 'h.25.mlp.c_proj.weight', 'h.21.ln_1.weight', 'h.47.mlp.c_fc.weight', 'h.23.attn.c_attn.weight', 'h.32.mlp.c_fc.weight', 'h.31.ln_2.weight', 'h.19.mlp.c_fc.bias', 'h.30.ln_1.weight', 'h.10.attn.c_attn.bias', 'h.42.mlp.c_fc.weight', 'h.27.attn.c_attn.weight', 'h.36.ln_2.bias', 'h.37.mlp.c_fc.bias', 'h.39.mlp.c_fc.bias', 'h.26.mlp.c_fc.bias', 'h.7.attn.c_proj.bias', 'h.30.attn.c_proj.weight', 'h.34.ln_2.bias', 'h.38.attn.c_proj.bias', 'h.2.ln_1.bias', 'h.13.attn.c_attn.bias', 'h.31.attn.c_attn.bias', 'h.37.ln_2.bias', 'h.11.mlp.c_proj.bias', 'h.31.mlp.c_fc.bias', 'h.46.attn.c_attn.bias', 'h.5.ln_2.weight', 'h.36.ln_1.bias', 'h.2.mlp.c_fc.weight', 'h.20.mlp.c_proj.weight', 'h.24.attn.c_attn.bias', 'h.10.attn.c_proj.bias', 'h.36.mlp.c_proj.weight', 'h.30.ln_1.bias', 'h.29.mlp.c_proj.bias', 'h.21.ln_2.bias', 'h.32.ln_2.weight', 'h.7.mlp.c_proj.bias', 'h.22.mlp.c_fc.weight', 'h.20.mlp.c_fc.bias', 'h.36.attn.c_attn.weight', 'h.8.mlp.c_proj.bias', 'h.9.mlp.c_proj.weight', 'h.47.ln_1.bias', 'h.34.mlp.c_proj.weight', 'h.22.attn.c_attn.weight', 'h.33.mlp.c_fc.bias', 'h.6.mlp.c_fc.bias', 'h.1.ln_2.weight', 'h.6.mlp.c_proj.weight', 'h.15.mlp.c_proj.weight', 'h.3.mlp.c_proj.bias', 'h.35.attn.c_proj.weight', 'h.33.ln_1.bias', 'h.16.attn.c_attn.weight', 'h.17.ln_2.bias', 'h.20.attn.c_proj.weight', 'h.17.ln_2.weight', 'h.3.attn.c_attn.bias', 'h.45.ln_2.bias', 'h.7.ln_2.bias', 'h.25.ln_2.bias', 'h.7.attn.c_proj.weight', 'h.14.ln_1.bias', 'h.37.ln_1.bias', 'h.6.attn.c_attn.bias', 'h.17.attn.c_attn.bias', 'h.21.mlp.c_proj.bias', 'h.31.mlp.c_fc.weight', 'h.44.mlp.c_fc.bias', 'h.11.ln_2.bias', 'h.4.attn.c_attn.bias', 'h.43.ln_2.weight', 'h.31.mlp.c_proj.bias', 'h.13.ln_1.weight', 'h.2.ln_2.weight', 'h.11.ln_1.bias', 'h.11.mlp.c_fc.bias', 'h.38.attn.c_attn.bias', 'h.27.ln_2.bias', 'h.7.ln_1.weight', 'h.25.ln_1.bias', 'h.26.ln_1.bias', 'h.41.ln_2.weight', 'h.31.ln_2.bias', 'h.0.ln_1.weight', 'h.13.ln_1.bias', 'h.27.ln_1.weight', 'h.46.attn.c_proj.weight', 'h.27.ln_2.weight', 'h.15.attn.c_proj.bias', 'h.16.attn.c_proj.weight', 'h.4.mlp.c_fc.weight', 'h.24.ln_1.bias', 'h.30.attn.c_attn.bias', 'h.43.ln_1.weight', 'h.35.mlp.c_proj.bias', 'h.1.mlp.c_fc.weight', 'h.22.ln_1.bias', 'h.27.attn.c_attn.bias', 'h.44.attn.c_attn.weight', 'h.3.attn.c_attn.weight', 'h.43.ln_1.bias', 'h.20.attn.c_proj.bias', 'h.42.ln_1.bias', 'h.34.ln_1.bias', 'h.23.mlp.c_proj.bias', 'h.10.attn.c_attn.weight', 'h.8.ln_2.weight', 'h.33.attn.c_attn.weight', 'h.12.attn.c_attn.weight', 'h.35.ln_1.bias', 'h.15.ln_2.weight', 'h.44.ln_1.bias', 'h.12.mlp.c_fc.weight', 'h.18.ln_1.weight', 'h.45.mlp.c_proj.bias', 'h.35.attn.c_attn.bias', 'h.42.ln_2.bias', 'h.26.mlp.c_fc.weight', 'h.1.ln_1.weight', 'h.16.mlp.c_fc.weight', 'h.12.attn.c_proj.weight', 'h.15.ln_1.bias', 'h.19.attn.c_attn.weight', 'h.46.attn.c_proj.bias', 'h.17.ln_1.weight', 'h.19.mlp.c_fc.weight', 'h.9.attn.c_proj.weight', 'h.31.ln_1.weight', 'h.20.ln_2.bias', 'h.43.attn.c_attn.weight', 'h.39.ln_2.bias', 'h.23.mlp.c_proj.weight', 'h.26.attn.c_attn.weight', 'h.21.ln_2.weight', 'h.7.attn.c_attn.bias', 'h.9.attn.c_attn.weight', 'h.9.attn.c_proj.bias', 'h.33.attn.c_proj.bias', 'h.30.attn.c_proj.bias', 'h.8.attn.c_proj.bias', 'h.47.attn.c_proj.weight', 'h.26.mlp.c_proj.bias', 'h.42.attn.c_attn.bias', 'h.0.mlp.c_fc.bias', 'h.7.ln_2.weight', 'h.0.mlp.c_fc.weight', 'h.9.mlp.c_fc.weight', 'h.41.mlp.c_proj.weight', 'h.1.attn.c_attn.weight', 'h.45.attn.c_proj.bias', 'h.6.ln_1.bias', 'h.23.mlp.c_fc.weight', 'h.40.mlp.c_fc.bias', 'h.10.ln_2.bias', 'h.19.ln_1.bias', 'h.21.mlp.c_proj.weight', 'h.41.mlp.c_proj.bias', 'h.19.ln_2.bias', 'h.30.attn.c_attn.weight', 'h.11.attn.c_proj.bias', 'h.15.mlp.c_proj.bias', 'h.39.ln_1.bias', 'h.3.mlp.c_fc.weight', 'h.21.attn.c_attn.bias', 'h.32.attn.c_proj.weight', 'h.43.mlp.c_proj.bias', 'h.33.attn.c_proj.weight', 'h.2.ln_2.bias', 'h.29.mlp.c_proj.weight', 'h.10.ln_1.bias', 'h.2.ln_1.weight', 'h.25.attn.c_proj.weight', 'h.45.ln_2.weight', 'h.22.ln_1.weight', 'h.12.ln_1.bias', 'h.14.mlp.c_proj.bias', 'h.38.mlp.c_fc.bias', 'h.10.ln_2.weight', 'h.6.attn.c_proj.bias', 'h.6.ln_2.weight', 'h.28.attn.c_proj.weight', 'h.27.mlp.c_fc.weight', 'h.17.mlp.c_fc.weight', 'h.4.ln_1.bias', 'h.23.ln_2.bias', 'h.14.attn.c_proj.bias', 'h.24.mlp.c_proj.weight', 'h.31.ln_1.bias', 'h.19.ln_1.weight', 'h.10.mlp.c_proj.weight', 'h.3.ln_1.weight', 'h.22.ln_2.bias', 'h.26.mlp.c_proj.weight', 'h.33.ln_2.weight', 'h.36.ln_1.weight', 'h.39.mlp.c_proj.weight', 'h.32.ln_2.bias', 'h.31.attn.c_proj.weight', 'h.8.attn.c_attn.weight', 'h.6.ln_2.bias', 'h.1.ln_2.bias', 'h.5.mlp.c_fc.weight', 'h.34.mlp.c_fc.bias', 'h.20.attn.c_attn.bias', 'h.28.mlp.c_fc.weight', 'h.32.mlp.c_fc.bias', 'h.28.mlp.c_proj.bias', 'h.21.mlp.c_fc.weight', 'h.31.mlp.c_proj.weight', 'h.5.ln_1.weight', 'h.20.ln_1.bias', 'h.19.mlp.c_proj.weight', 'h.19.attn.c_proj.weight', 'h.16.mlp.c_proj.bias', 'h.30.mlp.c_fc.bias', 'h.18.mlp.c_proj.weight', 'h.28.attn.c_attn.bias', 'h.39.mlp.c_fc.weight', 'h.29.attn.c_attn.weight', 'h.41.ln_1.bias', 'h.36.mlp.c_proj.bias', 'h.3.ln_2.bias', 'h.17.attn.c_proj.weight', 'h.38.mlp.c_fc.weight', 'h.5.ln_2.bias', 'h.44.attn.c_attn.bias', 'h.45.ln_1.bias', 'h.43.mlp.c_fc.bias', 'h.28.ln_2.weight', 'h.17.mlp.c_proj.bias', 'h.44.ln_2.weight', 'h.24.mlp.c_fc.weight', 'h.32.ln_1.weight', 'h.30.mlp.c_proj.weight', 'h.42.attn.c_proj.bias', 'h.4.mlp.c_proj.bias', 'h.1.attn.c_proj.bias', 'h.40.attn.c_attn.bias', 'h.18.attn.c_proj.weight', 'h.5.attn.c_proj.weight', 'h.11.ln_1.weight', 'h.33.ln_2.bias', 'h.4.mlp.c_fc.bias', 'h.37.ln_1.weight', 'h.16.ln_2.bias', 'h.22.attn.c_proj.bias', 'h.34.attn.c_attn.weight', 'h.26.attn.c_proj.bias', 'h.26.attn.c_proj.weight', 'h.19.ln_2.weight', 'h.40.attn.c_attn.weight', 'h.24.ln_2.weight', 'h.9.ln_2.bias', 'h.2.attn.c_proj.bias', 'h.0.mlp.c_proj.bias', 'h.8.ln_1.weight', 'h.32.mlp.c_proj.weight', 'h.5.attn.c_attn.weight', 'h.19.attn.c_proj.bias', 'h.5.mlp.c_proj.weight', 'h.42.mlp.c_fc.bias', 'h.8.mlp.c_proj.weight', 'h.35.mlp.c_proj.weight', 'h.20.ln_2.weight', 'h.42.mlp.c_proj.bias', 'h.8.attn.c_proj.weight', 'h.37.ln_2.weight', 'h.18.attn.c_attn.weight', 'h.40.mlp.c_proj.bias', 'h.39.ln_1.weight', 'h.2.mlp.c_fc.bias', 'h.42.attn.c_proj.weight', 'h.44.mlp.c_fc.weight', 'h.20.attn.c_attn.weight', 'h.17.mlp.c_proj.weight', 'h.6.ln_1.weight', 'h.16.mlp.c_proj.weight', 'h.34.attn.c_proj.weight', 'h.32.ln_1.bias', 'h.38.mlp.c_proj.weight', 'h.0.attn.c_proj.weight', 'h.9.ln_2.weight', 'h.16.attn.c_proj.bias', 'h.28.ln_1.bias', 'h.26.attn.c_attn.bias', 'h.35.ln_2.bias', 'h.13.mlp.c_proj.weight', 'h.0.mlp.c_proj.weight', 'h.38.attn.c_proj.weight', 'h.2.attn.c_attn.weight', 'h.43.attn.c_attn.bias', 'h.40.mlp.c_fc.weight', 'h.5.mlp.c_proj.bias', 'h.14.ln_1.weight', 'h.18.mlp.c_fc.weight', 'h.25.attn.c_proj.bias', 'h.25.mlp.c_fc.bias', 'h.9.mlp.c_proj.bias', 'h.46.attn.c_attn.weight', 'h.30.mlp.c_proj.bias', 'h.41.ln_2.bias', 'h.40.attn.c_proj.bias', 'h.32.attn.c_proj.bias', 'h.0.ln_1.bias', 'h.17.ln_1.bias', 'h.27.attn.c_proj.weight', 'h.1.attn.c_proj.weight', 'h.36.ln_2.weight', 'h.0.attn.c_attn.bias', 'h.12.attn.c_proj.bias', 'h.15.mlp.c_fc.bias', 'h.41.ln_1.weight', 'h.28.ln_1.weight', 'h.41.attn.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.4.ln_2.bias', 'h.25.mlp.c_proj.bias', 'h.13.attn.c_proj.bias', 'h.28.attn.c_attn.weight', 'h.20.ln_1.weight', 'h.42.mlp.c_proj.weight', 'h.23.ln_1.weight', 'h.15.attn.c_attn.bias', 'h.23.attn.c_proj.weight', 'h.12.ln_2.weight', 'h.41.attn.c_attn.bias', 'h.6.mlp.c_fc.weight', 'h.45.attn.c_attn.bias', 'h.22.attn.c_proj.weight', 'h.1.ln_1.bias', 'h.40.mlp.c_proj.weight', 'h.36.mlp.c_fc.bias', 'ln_f.weight', 'h.20.mlp.c_fc.weight', 'h.36.attn.c_proj.bias', 'h.15.attn.c_attn.weight', 'h.43.attn.c_proj.bias', 'h.5.attn.c_proj.bias', 'h.13.attn.c_attn.weight', 'h.24.ln_1.weight', 'h.33.mlp.c_proj.weight', 'h.23.ln_2.weight', 'h.38.mlp.c_proj.bias', 'h.46.ln_1.bias', 'h.30.ln_2.bias', 'h.11.mlp.c_fc.weight', 'h.10.attn.c_proj.weight', 'h.24.mlp.c_proj.bias', 'h.37.mlp.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.13.mlp.c_fc.weight', 'h.4.ln_2.weight', 'h.14.attn.c_attn.bias', 'h.40.ln_1.bias', 'h.11.mlp.c_proj.weight', 'h.27.ln_1.bias', 'h.21.ln_1.bias', 'h.25.mlp.c_fc.weight', 'h.13.attn.c_proj.weight', 'h.18.ln_1.bias', 'h.10.mlp.c_fc.weight', 'h.34.attn.c_proj.bias', 'h.23.ln_1.bias', 'h.16.ln_1.weight', 'h.47.attn.c_attn.bias', 'h.16.attn.c_attn.bias', 'h.7.mlp.c_fc.weight', 'h.46.mlp.c_proj.weight', 'h.46.ln_2.weight', 'h.12.mlp.c_fc.bias', 'h.6.attn.c_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d265cfbcf3474db3887dfe1b51afa26c"}},"metadata":{}}]},{"cell_type":"code","source":"def read_input_file(file_path):\n    with open(file_path, 'r') as file:\n        return file.read().strip()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:46.468615Z","iopub.execute_input":"2023-09-11T01:37:46.469355Z","iopub.status.idle":"2023-09-11T01:37:46.476032Z","shell.execute_reply.started":"2023-09-11T01:37:46.469313Z","shell.execute_reply":"2023-09-11T01:37:46.474757Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"file_path = \"/kaggle/input/full-texts/Abstract-2529.txt\"\ninput_text = read_input_file(file_path)\ninput_text = input_text","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:46.479695Z","iopub.execute_input":"2023-09-11T01:37:46.480104Z","iopub.status.idle":"2023-09-11T01:37:46.512010Z","shell.execute_reply.started":"2023-09-11T01:37:46.480063Z","shell.execute_reply":"2023-09-11T01:37:46.510858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer.encode_plus(input_text, return_tensors='pt', truncation=False, padding='longest')\ninput_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n\n# pad the input_ids and attention_mask with the new padding token\nmax_length = 100\npadded_input_ids = input_ids[:, :max_length].reshape(-1, max_length)\npadded_attention_mask = attention_mask[:, :max_length].reshape(-1, max_length)\n\nsummary_ids = model.generate(padded_input_ids, attention_mask=padded_attention_mask, early_stopping=False, min_length=90, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n\nGPT_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\nprint(GPT_summary)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:46.513114Z","iopub.execute_input":"2023-09-11T01:37:46.513523Z","iopub.status.idle":"2023-09-11T01:37:49.652247Z","shell.execute_reply.started":"2023-09-11T01:37:46.513499Z","shell.execute_reply":"2023-09-11T01:37:49.651153Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"We report a rare case of immunoglobulin G4 IgG4-related sclerosing cholangitis without other organ involvement. A 69-year-old-man was referred for the evaluation of jaundice. Computed tomography revealed thickening of pigmentosa</w>\n","output_type":"stream"}]},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\ndef compute_rouge(reference, candidate):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n    scores = scorer.score(reference, candidate)\n    return scores\n\nif __name__ == '__main__':\n    file_path = \"/kaggle/input/full-texts/Abstract-2529.txt\"\n    input_abstract = read_input_file(file_path)\n    reference =  GPT_summary \n    candidate = input_abstract\n\n    scores = compute_rouge(reference, candidate)\n    for key, score in scores.items():\n        print(f\"{key.upper()}:\")\n        print(f\"  Precision: {score.precision:.4f}\")\n        print(f\"  Recall: {score.recall:.4f}\")\n        print(f\"  F1 Score: {score.fmeasure:.4f}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:49.653341Z","iopub.execute_input":"2023-09-11T01:37:49.653648Z","iopub.status.idle":"2023-09-11T01:37:51.222573Z","shell.execute_reply.started":"2023-09-11T01:37:49.653623Z","shell.execute_reply":"2023-09-11T01:37:51.221209Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ROUGE1:\n  Precision: 0.2000\n  Recall: 0.9429\n  F1 Score: 0.3300\n\nROUGE2:\n  Precision: 0.1951\n  Recall: 0.9412\n  F1 Score: 0.3232\n\nROUGEL:\n  Precision: 0.2000\n  Recall: 0.9429\n  F1 Score: 0.3300\n\nROUGELSUM:\n  Precision: 0.2000\n  Recall: 0.9429\n  F1 Score: 0.3300\n\n","output_type":"stream"}]},{"cell_type":"code","source":"del model","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:51.223845Z","iopub.execute_input":"2023-09-11T01:37:51.224146Z","iopub.status.idle":"2023-09-11T01:37:51.484615Z","shell.execute_reply.started":"2023-09-11T01:37:51.224123Z","shell.execute_reply":"2023-09-11T01:37:51.483427Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:51.486056Z","iopub.execute_input":"2023-09-11T01:37:51.486327Z","iopub.status.idle":"2023-09-11T01:37:51.491773Z","shell.execute_reply.started":"2023-09-11T01:37:51.486305Z","shell.execute_reply":"2023-09-11T01:37:51.490103Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the model and tokenizer\nMODEL_NAME = \"stanford-crfm/BioMedLM\"  # or whatever the model's name is on Hugging Face Model Hub\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:37:51.493562Z","iopub.execute_input":"2023-09-11T01:37:51.493950Z","iopub.status.idle":"2023-09-11T01:41:18.114008Z","shell.execute_reply.started":"2023-09-11T01:37:51.493920Z","shell.execute_reply":"2023-09-11T01:41:18.112792Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/602k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab32c80d3d224661b5b248591db6b924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/276k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93abd30ddc6433393784748b91e9103"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/267 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b599472baf3842a18286456337247a7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/876 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2066ebee87c47589d660a3d7b394bf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/10.7G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de2efd60b3a457cb99593bce77c2491"}},"metadata":{}}]},{"cell_type":"code","source":"def ask_contextual_question(context, question, model, tokenizer, max_length=200):\n    combined_input = context + \" \" + question\n    input_ids = tokenizer.encode(combined_input, return_tensors='pt')\n\n    # Generate response\n    with torch.no_grad():\n        output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n\n    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True).replace(combined_input, '').strip()\n    return decoded_output\n\n# Use the function\nquestion = \"What happened to the bile duct? based on the context provided above\"\nresponse = ask_contextual_question(GPT_summary, question, model, tokenizer)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:41:18.117274Z","iopub.execute_input":"2023-09-11T01:41:18.117615Z","iopub.status.idle":"2023-09-11T01:43:34.582050Z","shell.execute_reply.started":"2023-09-11T01:41:18.117586Z","shell.execute_reply":"2023-09-11T01:43:34.580025Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":", we diagnosed IgG4-related sclerosing cholangitis. The patient was treated with prednisolone (30 mg/day) and ursodeoxycholic acid (600 mg\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('pubmed_qa', 'pqa_labeled')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T01:43:34.583828Z","iopub.execute_input":"2023-09-11T01:43:34.584147Z","iopub.status.idle":"2023-09-11T01:43:49.599392Z","shell.execute_reply.started":"2023-09-11T01:43:34.584121Z","shell.execute_reply":"2023-09-11T01:43:49.597648Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd0e2be60864be0a240f0537fbb8862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28650fd3a1a044a5a68549834488b0e2"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset pubmed_qa/pqa_labeled (download: 656.02 MiB, generated: 1.99 MiB, post-processed: Unknown size, total: 658.01 MiB) to /root/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e7465d0db0b443c8a687621046d7fc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/709k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83e5868ece104fae965395199dd44f0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/152M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43307454c774744ae2f001b7c122e21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/533M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f530afc84ef04f8fb932f56f10e467f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42cd69f79f2a44e2a377af6ebc095cd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset pubmed_qa downloaded and prepared to /root/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23abef9f27d4e03be93deed9d6d06d5"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-09-11T02:39:35.747539Z","iopub.execute_input":"2023-09-11T02:39:35.747925Z","iopub.status.idle":"2023-09-11T02:39:35.756043Z","shell.execute_reply.started":"2023-09-11T02:39:35.747899Z","shell.execute_reply":"2023-09-11T02:39:35.754785Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'pubid': 21645374,\n 'question': 'Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?',\n 'context': {'contexts': ['Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.',\n   'The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.'],\n  'labels': ['BACKGROUND', 'RESULTS'],\n  'meshes': ['Alismataceae',\n   'Apoptosis',\n   'Cell Differentiation',\n   'Mitochondria',\n   'Plant Leaves'],\n  'reasoning_required_pred': ['y', 'e', 's'],\n  'reasoning_free_pred': ['y', 'e', 's']},\n 'long_answer': 'Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of our knowledge, this is the first report of mitochondria and chloroplasts moving on transvacuolar strands to form a ring structure surrounding the nucleus during developmental PCD. Also, for the first time, we have shown the feasibility for the use of CsA in a whole plant system. Overall, our findings implicate the mitochondria as playing a critical and early role in developmentally regulated PCD in the lace plant.',\n 'final_decision': 'yes'}"},"metadata":{}}]},{"cell_type":"code","source":"total_f1, total_exact = 0, 0\n\nfor sample in dataset['train']):\n    question = sample['question']\n    context = \" \".join(sample['context']['contexts'])\n    correct_answer = sample['long_answer']\n\n    pred_answer = ask_contextual_question(context, question, model, tokenizer)\n\n    # Compute F1 and Exact match\n    common = set(pred_answer.lower().split()) & set(correct_answer.lower().split())\n    f1 = 2 * len(common) / (len(pred_answer.split()) + len(correct_answer.split()))\n    total_f1 += f1\n\n    if pred_answer.lower() == correct_answer.lower():\n        total_exact += 1","metadata":{"execution":{"iopub.status.busy":"2023-09-11T02:43:51.664747Z","iopub.execute_input":"2023-09-11T02:43:51.665185Z","iopub.status.idle":"2023-09-11T03:05:08.413412Z","shell.execute_reply.started":"2023-09-11T02:43:51.665153Z","shell.execute_reply":"2023-09-11T03:05:08.412412Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 437, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 345, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 295, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 379, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 336, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 303, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 255, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 342, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 223, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 523, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 315, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 360, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 343, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 328, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 302, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 509, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 363, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 196, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 276, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 378, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 215, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 375, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 380, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 218, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 341, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 357, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 219, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 234, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 307, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 279, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 154, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 431, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 275, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 353, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 349, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 412, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 435, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 414, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 203, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 340, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 318, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 313, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 212, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 367, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 324, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 226, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 402, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 294, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 265, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 133, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 344, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 208, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 301, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 108, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 305, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 192, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 451, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 325, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 233, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 401, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 405, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 352, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 189, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 242, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 228, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 254, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 259, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 384, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 430, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 224, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 445, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 555, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 151, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 436, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 440, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 426, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 485, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 396, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 403, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 350, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 333, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 227, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 284, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 443, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Average F1 Score: 0.0032246016124654457\nExact Match: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Average F1 and Exact match\navg_f1 = total_f1 / len(dataset['train'])\navg_exact = total_exact / len(dataset['train'])\n\nprint(f\"Average F1 Score: {avg_f1}\")\nprint(f\"Exact Match: {avg_exact}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:10:29.572984Z","iopub.execute_input":"2023-09-11T03:10:29.573487Z","iopub.status.idle":"2023-09-11T03:10:29.579314Z","shell.execute_reply.started":"2023-09-11T03:10:29.573451Z","shell.execute_reply":"2023-09-11T03:10:29.578404Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Average F1 Score: 0.032246016124654456\nExact Match: 0.0\n","output_type":"stream"}]}]}